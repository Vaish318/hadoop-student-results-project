"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"3d16a563\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Hadoop Student Results \u2014 Demonstration Notebook\\n\",\n    \"\\n\",\n    \"This notebook walks through a full Hadoop Streaming project on your Mac (local pseudo-distributed Hadoop).\\n\",\n    \"\\n\",\n    \"**What it contains:**\\n\",\n    \"- dataset generation\\n\",\n    \"- HDFS upload\\n\",\n    \"- mapper/reducer code (streaming)\\n\",\n    \"- running MapReduce jobs using Hadoop Streaming (from within the notebook)\\n\",\n    \"- collecting outputs into pandas DataFrames\\n\",\n    \"- visualizations and exportable results for your examiner.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"b288b4c0\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Prerequisites\\n\",\n    \"Make sure before running this notebook:  \\n\",\n    \"- Hadoop is installed and configured (HADOOP_HOME set).\\n\",\n    \"- HDFS and YARN daemons are running (NameNode, DataNode, ResourceManager, NodeManager).\\n\",\n    \"- `hadoop` and `hdfs` commands work from your shell.\\n\",\n    \"- You can run shell commands in notebook cells with leading `!`.\\n\",\n    \"\\n\",\n    \"We'll detect `HADOOP_HOME` and the streaming JAR automatically in a cell below.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"c501af8c\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os, glob, subprocess, shlex\\n\",\n    \"print('USER:', os.environ.get('USER'))\\n\",\n    \"HADOOP_HOME = os.environ.get('HADOOP_HOME')\\n\",\n    \"print('HADOOP_HOME ->', HADOOP_HOME)\\n\",\n    \"\\n\",\n    \"streaming_pattern = None\\n\",\n    \"if HADOOP_HOME:\\n\",\n    \"    streaming_pattern = os.path.join(HADOOP_HOME, 'share','hadoop','tools','lib','hadoop-streaming-*.jar')\\n\",\n    \"    jars = glob.glob(streaming_pattern)\\n\",\n    \"else:\\n\",\n    \"    jars = []\\n\",\n    \"\\n\",\n    \"print('streaming_pattern ->', streaming_pattern)\\n\",\n    \"print('found streaming jars ->', jars)\\n\",\n    \"\\n\",\n    \"# quick hadoop version + jps\\n\",\n    \"try:\\n\",\n    \"    print('\\n\",\n    \"--- hadoop version ---')\\n\",\n    \"    print(subprocess.check_output(['hadoop','version']).decode().splitlines()[0])\\n\",\n    \"except Exception as e:\\n\",\n    \"    print('hadoop version: could not run hadoop ->', e)\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    print('\\n\",\n    \"--- jps (Java processes) ---')\\n\",\n    \"    print(subprocess.check_output(['jps','-l']).decode())\\n\",\n    \"except Exception as e:\\n\",\n    \"    print('jps: could not run jps ->', e)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"399f7c1c\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Generate dataset (change NUM_STUDENTS as needed)\\n\",\n    \"from random import choice, randint\\n\",\n    \"import csv\\n\",\n    \"NUM_STUDENTS = 1000\\n\",\n    \"SUBJECTS = [\\\"Math\\\",\\\"English\\\",\\\"Science\\\",\\\"Physics\\\",\\\"Chemistry\\\",\\\"Biology\\\",\\\"Computer\\\"]\\n\",\n    \"names = [\\\"Alice\\\",\\\"Bob\\\",\\\"Charlie\\\",\\\"Diana\\\",\\\"Eva\\\",\\\"Frank\\\",\\\"Grace\\\",\\\"Henry\\\",\\\"Ivy\\\",\\\"Jack\\\",\\n\",\n    \"         \\\"Karan\\\",\\\"Liya\\\",\\\"Manoj\\\",\\\"Nisha\\\",\\\"Om\\\",\\\"Priya\\\",\\\"Qadir\\\",\\\"Riya\\\",\\\"Sanjay\\\",\\\"Tara\\\"]\\n\",\n    \"\\n\",\n    \"with open('student_marks.csv','w',newline='') as f:\\n\",\n    \"    w = csv.writer(f)\\n\",\n    \"    w.writerow(['student_id','name','class','subject','marks'])\\n\",\n    \"    for i in range(1, NUM_STUDENTS+1):\\n\",\n    \"        sid = f'S{i:05d}'\\n\",\n    \"        name = choice(names)\\n\",\n    \"        cls = str(choice([9,10,11,12]))\\n\",\n    \"        # write one row per subject (balanced)\\n\",\n    \"        for subj in SUBJECTS:\\n\",\n    \"            marks = randint(30,100)\\n\",\n    \"            w.writerow([sid, name, cls, subj, marks])\\n\",\n    \"\\n\",\n    \"print('Wrote student_marks.csv with', NUM_STUDENTS * len(SUBJECTS), 'rows')\\n\",\n    \"!ls -lh student_marks.csv\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"1830a572\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Upload dataset to HDFS (overwrite if exists). Run only if HDFS is running.\\n\",\n    \"import os\\n\",\n    \"hdfs_dir = f\\\"/user/{os.environ.get('USER')}/student_data\\\"\\n\",\n    \"print('HDFS dest ->', hdfs_dir)\\n\",\n    \"\\n\",\n    \"# create dir and put file\\n\",\n    \"!hdfs dfs -mkdir -p {hdfs_dir} || true\\n\",\n    \"!hdfs dfs -put -f student_marks.csv {hdfs_dir}/\\n\",\n    \"!hdfs dfs -ls -h {hdfs_dir}\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"5c081fac\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Mapper: outputs \\\"subject\\tmarks\\\"\\n\",\n    \"mapper_subject = r\\\"\\\"\\\"#!/usr/bin/env python3\\n\",\n    \"import sys\\n\",\n    \"import csv\\n\",\n    \"reader = csv.DictReader(sys.stdin)\\n\",\n    \"for row in reader:\\n\",\n    \"    try:\\n\",\n    \"        subject = row.get('subject','').strip()\\n\",\n    \"        marks = int(row.get('marks','').strip())\\n\",\n    \"        if subject:\\n\",\n    \"            print(f\\\"{subject}\\t{marks}\\\")\\n\",\n    \"    except Exception:\\n\",\n    \"        continue\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"open('mapper_subject.py','w').write(mapper_subject)\\n\",\n    \"!chmod +x mapper_subject.py\\n\",\n    \"\\n\",\n    \"# Reducer: computes count, sum, max, avg per subject\\n\",\n    \"reducer_subject = r\\\"\\\"\\\"#!/usr/bin/env python3\\n\",\n    \"import sys\\n\",\n    \"current = None\\n\",\n    \"count = 0\\n\",\n    \"_sum = 0\\n\",\n    \"_max = None\\n\",\n    \"\\n\",\n    \"def emit(s,c,t,m):\\n\",\n    \"    avg = t / c if c else 0\\n\",\n    \"    print(f\\\"{s}\\t{c}\\t{t}\\t{m}\\t{avg:.2f}\\\")\\n\",\n    \"\\n\",\n    \"for line in sys.stdin:\\n\",\n    \"    line = line.strip()\\n\",\n    \"    if not line:\\n\",\n    \"        continue\\n\",\n    \"    parts = line.split('\\t',1)\\n\",\n    \"    if len(parts) != 2: continue\\n\",\n    \"    subj, val = parts\\n\",\n    \"    try:\\n\",\n    \"        marks = int(val)\\n\",\n    \"    except:\\n\",\n    \"        continue\\n\",\n    \"    if current is None:\\n\",\n    \"        current = subj; count = 0; _sum = 0; _max = marks\\n\",\n    \"    if subj != current:\\n\",\n    \"        emit(current,count,_sum,_max)\\n\",\n    \"        current = subj; count = 0; _sum = 0; _max = marks\\n\",\n    \"    count += 1\\n\",\n    \"    _sum += marks\\n\",\n    \"    if _max is None or marks > _max:\\n\",\n    \"        _max = marks\\n\",\n    \"\\n\",\n    \"if current is not None:\\n\",\n    \"    emit(current,count,_sum,_max)\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"open('reducer_subject.py','w').write(reducer_subject)\\n\",\n    \"!chmod +x reducer_subject.py\\n\",\n    \"!ls -l mapper_subject.py reducer_subject.py\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"72d82994\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Run Hadoop Streaming job: per-subject stats (adjust streaming jar path if needed)\\n\",\n    \"import os, glob\\n\",\n    \"HADOOP_STREAMING = glob.glob(os.path.join(os.environ.get('HADOOP_HOME',''),'share','hadoop','tools','lib','hadoop-streaming-*.jar'))\\n\",\n    \"if not HADOOP_STREAMING:\\n\",\n    \"    raise RuntimeError('Could not find hadoop-streaming jar. Check HADOOP_HOME.')\\n\",\n    \"STREAMING_JAR = HADOOP_STREAMING[0]\\n\",\n    \"print('Using streaming jar ->', STREAMING_JAR)\\n\",\n    \"\\n\",\n    \"!hdfs dfs -rm -r -f /user/$USER/output_subject_stats || true\\n\",\n    \"!hadoop jar \\\"{STREAMING_JAR}\\\" -files mapper_subject.py,reducer_subject.py -mapper mapper_subject.py -reducer reducer_subject.py -numReduceTasks 2 -input /user/$USER/student_data/student_marks.csv -output /user/$USER/output_subject_stats\\n\",\n    \"\\n\",\n    \"!hdfs dfs -cat \\\"/user/$USER/output_subject_stats/part-*\\\" | sed -n '1,200p'\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"0856dd4f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Mapper: subject rows -> key: student|name, val: marks\\n\",\n    \"mapper_student = r\\\"\\\"\\\"#!/usr/bin/env python3\\n\",\n    \"import sys, csv\\n\",\n    \"reader = csv.DictReader(sys.stdin)\\n\",\n    \"for row in reader:\\n\",\n    \"    try:\\n\",\n    \"        sid = row.get('student_id','').strip()\\n\",\n    \"        name = row.get('name','').strip()\\n\",\n    \"        marks = int(row.get('marks','').strip())\\n\",\n    \"        print(f\\\"{sid}|{name}\\t{marks}\\\")\\n\",\n    \"    except Exception:\\n\",\n    \"        continue\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"open('mapper_student.py','w').write(mapper_student)\\n\",\n    \"!chmod +x mapper_student.py\\n\",\n    \"\\n\",\n    \"# Reducer: sum marks per student\\n\",\n    \"reducer_student = r\\\"\\\"\\\"#!/usr/bin/env python3\\n\",\n    \"import sys\\n\",\n    \"current = None\\n\",\n    \"_total = 0\\n\",\n    \"for line in sys.stdin:\\n\",\n    \"    line = line.strip()\\n\",\n    \"    if not line: continue\\n\",\n    \"    parts = line.split('\\t',1)\\n\",\n    \"    if len(parts)!=2: continue\\n\",\n    \"    key, val = parts\\n\",\n    \"    try:\\n\",\n    \"        marks = int(val)\\n\",\n    \"    except:\\n\",\n    \"        continue\\n\",\n    \"    if current is None:\\n\",\n    \"        current = key; _total = 0\\n\",\n    \"    if key != current:\\n\",\n    \"        sid, name = current.split('|',1)\\n\",\n    \"        print(f\\\"{sid}\\t{name}\\t{_total}\\\")\\n\",\n    \"        current = key; _total = 0\\n\",\n    \"    _total += marks\\n\",\n    \"if current is not None:\\n\",\n    \"    sid, name = current.split('|',1)\\n\",\n    \"    print(f\\\"{sid}\\t{name}\\t{_total}\\\")\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"open('reducer_student.py','w').write(reducer_student)\\n\",\n    \"!chmod +x reducer_student.py\\n\",\n    \"!ls -l mapper_student.py reducer_student.py\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"192bad6b\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Run Hadoop Streaming: student totals\\n\",\n    \"import glob, os\\n\",\n    \"HADOOP_STREAMING = glob.glob(os.path.join(os.environ.get('HADOOP_HOME',''),'share','hadoop','tools','lib','hadoop-streaming-*.jar'))\\n\",\n    \"STREAMING_JAR = HADOOP_STREAMING[0]\\n\",\n    \"!hdfs dfs -rm -r -f /user/$USER/output_student_totals || true\\n\",\n    \"!hadoop jar \\\"{STREAMING_JAR}\\\" -files mapper_student.py,reducer_student.py -mapper mapper_student.py -reducer reducer_student.py -numReduceTasks 4 -input /user/$USER/student_data/student_marks.csv -output /user/$USER/output_student_totals\\n\",\n    \"\\n\",\n    \"# fetch and show top results\\n\",\n    \"!hdfs dfs -cat \\\"/user/$USER/output_student_totals/part-*\\\" | sed -n '1,50p'\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"f3c683db\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Collect output_student_totals into a pandas DataFrame for analysis\\n\",\n    \"import subprocess, shlex, os\\n\",\n    \"from io import StringIO\\n\",\n    \"import pandas as pd\\n\",\n    \"\\n\",\n    \"cmd = f\\\"hdfs dfs -cat /user/{os.environ.get('USER')}/output_student_totals/part-*\\\"\\n\",\n    \"raw = subprocess.check_output(shlex.split(cmd)).decode()\\n\",\n    \"\\n\",\n    \"df = pd.read_csv(StringIO(raw), sep='\\t', header=None, names=['StudentID','Name','Total'])\\n\",\n    \"df['Total'] = pd.to_numeric(df['Total'], errors='coerce')\\n\",\n    \"df = df.dropna(subset=['Total'])\\n\",\n    \"\\n\",\n    \"df.head()\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"9aeb9f79\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Visualizations: histogram and grade distribution\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(8,4))\\n\",\n    \"plt.hist(df['Total'], bins=20)\\n\",\n    \"plt.title('Histogram of student totals')\\n\",\n    \"plt.xlabel('Total marks')\\n\",\n    \"plt.ylabel('Number of students')\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"# grade based on inferred max (see notebook earlier) -- here assume max=700 (7 subjects * 100)\\n\",\n    \"MAX_TOTAL = 7 * 100\\n\",\n    \"\\n\",\n    \"def grade(pct):\\n\",\n    \"    if pct >= 90: return 'A'\\n\",\n    \"    if pct >= 75: return 'B'\\n\",\n    \"    if pct >= 60: return 'C'\\n\",\n    \"    if pct >= 40: return 'D'\\n\",\n    \"    return 'F'\\n\",\n    \"\\n\",\n    \"df['Pct'] = df['Total'] / MAX_TOTAL * 100\\n\",\n    \"df['Grade'] = df['Pct'].apply(grade)\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(6,4))\\n\",\n    \"df['Grade'].value_counts().sort_index().plot(kind='bar')\\n\",\n    \"plt.title('Grade distribution (A-F)')\\n\",\n    \"plt.xlabel('Grade')\\n\",\n    \"plt.ylabel('Number of students')\\n\",\n    \"plt.show()\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"c92bfe6d\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Export top 50 students to CSV for submission\\n\",\n    \"out_csv = 'top50_students.csv'\\n\",\n    \"top50 = df.sort_values('Total', ascending=False).head(50)\\n\",\n    \"top50.to_csv(out_csv, index=False)\\n\",\n    \"print('Wrote', out_csv)\\n\",\n    \"!ls -lh top50_students.csv\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"097c4f56\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Screenshots (for your report)\\n\",\n    \"Below are two screenshots from the development run that you can include in your report.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"7e10a55f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from IPython.display import Image, display\\n\",\n    \"print('Screenshot 1:')\\n\",\n    \"display(Image(filename=r'/mnt/data/Screenshot 2025-11-21 at 18.55.49.png'))\\n\",\n    \"print('Screenshot 2:')\\n\",\n    \"display(Image(filename=r'/mnt/data/Screenshot 2025-11-21 at 18.55.57.png'))\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"716618cb\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Final checklist for your exam demo\\n\",\n    \"- Run the notebook cell-by-cell in order.\\n\",\n    \"- If HDFS/yarn are not running, start your Hadoop daemons first.\\n\",\n    \"- Show the `jps` output and Hadoop streaming job tracker URL (printed in job logs) to the examiner.\\n\",\n    \"- Export `top50_students.csv` and `student_totals_graded.csv` as deliverables.\\n\",\n    \"\\n\",\n    \"If you want, I can now:  \\n\",\n    \"- (A) produce the actual `.ipynb` here so you can download it, or  \\n\",\n    \"- (B) convert this generator into a ready-to-run notebook file and attach it.\\n\",\n    \"Tell me which you prefer.\"\n   ]\n  }\n ],\n \"metadata\": {},\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}"